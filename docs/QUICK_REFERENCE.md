# è‚¡ç¥¨äº¤æ˜“ç¯å¢ƒ - å¿«é€Ÿå‚è€ƒå¡

## ğŸ“‹ æ ¸å¿ƒå¯¹è±¡é€ŸæŸ¥è¡¨

### StockTradingEnv çŠ¶æ€å˜é‡
```python
env.balance           # å½“å‰ç°é‡‘
env.net_worth         # æ€»å‡€å€¼ï¼ˆç°é‡‘ + æŒè‚¡å¸‚å€¼ï¼‰
env.shares_held       # æŒæœ‰è‚¡æ•°
env.cost_basis        # å¹³å‡æŒè‚¡æˆæœ¬
env.current_step      # å½“å‰æ—¶é—´æ­¥
env.max_net_worth     # å†å²æœ€é«˜å‡€å€¼
```

### ç¯å¢ƒç©ºé—´
```python
action_space = Box([0, 0], [3, 1])           # [æ“ä½œç±»å‹ 0-3, æ¯”ä¾‹ 0-1]
observation_space = Box(-inf, inf, (6, 6))   # (ç‰¹å¾æ•°, çª—å£å¤§å°)
```

### æ“ä½œè§£é‡Š
| action_type èŒƒå›´ | æ“ä½œ | å‚æ•°å«ä¹‰ |
|-----------------|------|--------|
| 0.0 - 1.0 | ä¹°å…¥ | amount = ç”¨è´¦æˆ·ä½™é¢çš„æ¯”ä¾‹ |
| 1.0 - 2.0 | å–å‡º | amount = æŒæœ‰è‚¡æ•°çš„æ¯”ä¾‹ |
| 2.0 - 3.0 | æŒæœ‰ | æ— æ“ä½œ |

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨å‘½ä»¤

### æœ€å°åŒ–è®­ç»ƒè„šæœ¬
```python
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from env.StockTradingEnv import StockTradingEnv

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('./data/AAPL.csv').sort_values('Date')

# 2. åˆ›å»ºç¯å¢ƒ
env = DummyVecEnv([lambda: StockTradingEnv(df)])

# 3. è®­ç»ƒ
model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=20000)

# 4. ä¿å­˜
model.save('model')
```

### æ¨ç†è„šæœ¬
```python
from stable_baselines3 import PPO

model = PPO.load('model', env=env)
obs = env.reset()
for _ in range(100):
    action, _ = model.predict(obs)
    obs, reward, done, info = env.step(action)
    print(f"Reward: {reward}")
```

---

## ğŸ” è§‚å¯Ÿç©ºé—´è¯¦è§£

è§‚å¯Ÿæ˜¯ä¸€ä¸ª $(6, \text{window\_size})$ çš„çŸ©é˜µï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¡Œ 0: Open ä»·æ ¼ï¼ˆå½’ä¸€åŒ–ï¼‰        â”‚
â”‚ è¡Œ 1: High ä»·æ ¼ï¼ˆå½’ä¸€åŒ–ï¼‰        â”‚
â”‚ è¡Œ 2: Low ä»·æ ¼ï¼ˆå½’ä¸€åŒ–ï¼‰         â”‚
â”‚ è¡Œ 3: Close ä»·æ ¼ï¼ˆå½’ä¸€åŒ–ï¼‰       â”‚
â”‚ è¡Œ 4: Volumeï¼ˆå½’ä¸€åŒ–ï¼‰           â”‚
â”‚ è¡Œ 5: è´¦æˆ·ç»Ÿè®¡ï¼ˆ6 ä¸ªç‰¹å¾ï¼‰       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     åˆ— 0  åˆ— 1  åˆ— 2  ...  åˆ— N
     â†“     â†“     â†“           â†“
   Day 0 Day 1 Day 2 ...  Day N-1
```

**è¡Œ 5 çš„ 6 ä¸ªè´¦æˆ·ç‰¹å¾**ï¼š
```python
[balance/initial, max_net_worth/initial, shares_held, 
 cost_basis, total_shares_sold, total_sales_value]
```

---

## ğŸ“Š å¸¸ç”¨æ€§èƒ½æŒ‡æ ‡

### 1. æ”¶ç›Šç‡ (Return)
$$\text{Return} = \frac{\text{Final NetWorth} - \text{Initial Balance}}{\text{Initial Balance}} \times 100\%$$

### 2. æœ€å¤§å›æ’¤ (Max Drawdown)
$$\text{MDD} = \frac{\text{Max Equity} - \text{Equity}_{\text{trough}}}{\text{Max Equity}} \times 100\%$$

### 3. å¤æ™®æ¯”ç‡ (Sharpe Ratio)
$$\text{Sharpe} = \frac{\text{Mean Return} - \text{Risk-Free Rate}}{\text{Return Std Dev}}$$

### 4. èƒœç‡ (Win Rate)
$$\text{Win Rate} = \frac{\text{Profitable Steps}}{\text{Total Steps}} \times 100\%$$

### 5. å¹³å‡å¥–åŠ± (Mean Reward)
$$\bar{r} = \frac{1}{n}\sum_{i=1}^{n} r_i$$

---

## âš™ï¸ è¶…å‚æ•°å‚è€ƒè¡¨

| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|--------|------|
| `learning_rate` | 1e-5 ~ 1e-3 | è¶Šå°æ”¶æ•›è¶Šæ…¢ä½†è¶Šç¨³å®š |
| `batch_size` | 32 ~ 256 | è¶Šå¤§è¶Šç¨³å®šä½†éœ€è¦æ›´å¤šå†…å­˜ |
| `n_epochs` | 3 ~ 20 | PPO å†…å¾ªç¯æ¬¡æ•° |
| `gamma` | 0.9 ~ 0.999 | æŠ˜æ‰£å› å­ï¼Œè¶Šæ¥è¿‘ 1 è¶Šçœ‹é‡é•¿æœŸ |
| `gae_lambda` | 0.9 ~ 0.99 | GAE ç³»æ•° |
| `ent_coef` | 1e-4 ~ 1e-1 | ç†µå¥–åŠ±ç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢ |
| `window_size` | 2 ~ 20 | è§‚å¯Ÿçª—å£ï¼Œå¤ªå°å¤±å»å†å²ï¼Œå¤ªå¤§è®¡ç®—é‡å¤§ |
| `initial_balance` | 1k ~ 100k | åˆå§‹èµ„é‡‘ |

---

## ğŸ› å¿«é€Ÿæ’æŸ¥è¡¨

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|--------|
| **æ•°æ®é—®é¢˜** | NaN/Inf å¼‚å¸¸ | `df.dropna()`, `df[df['Close']>0]` |
| **ç¯å¢ƒé—®é¢˜** | step() è¿”å›å¼‚å¸¸ | æ£€æŸ¥ OHLCV åˆ—ï¼ŒéªŒè¯å½’ä¸€åŒ– |
| **æ¨¡å‹ä¸å­¦ä¹ ** | å¥–åŠ± = 0 | å¢åŠ  timestepsï¼Œè°ƒæ•´å­¦ä¹ ç‡ |
| **è¿‡åº¦æ‹Ÿåˆ** | è®­ç»ƒå¥½ï¼Œæµ‹è¯•å·® | åŠ å…¥éªŒè¯é›†ï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§ |
| **è®­ç»ƒè¿‡æ…¢** | æ—¶é—´å¤ªé•¿ | ä½¿ç”¨ SubprocVecEnvï¼Œå‡å°‘ window_size |
| **å†…å­˜æº¢å‡º** | OOM é”™è¯¯ | å‡å°‘ batch_sizeï¼Œä½¿ç”¨è¾ƒçŸ­çš„æ—¶é—´åºåˆ— |
| **ä»·æ ¼å¼‚å¸¸** | å‡ºç°æç«¯å€¼ | æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰åˆ†è‚¡/å¹¶è‚¡äº‹ä»¶ |

---

## ğŸ“ˆ è®­ç»ƒæ£€æŸ¥æ¸…å•

- [ ] æ•°æ®å·²åŠ è½½å¹¶éªŒè¯ï¼ˆéç©ºï¼Œä»·æ ¼ > 0ï¼‰
- [ ] ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼ˆaction_space å’Œ obs_space æ­£ç¡®ï¼‰
- [ ] ç¬¬ä¸€æ­¥æ¨ç†å¯ä»¥è¿è¡Œï¼ˆæ— é”™è¯¯ï¼‰
- [ ] æ¨¡å‹å¼€å§‹è®­ç»ƒï¼ˆçœ‹åˆ°æ—¥å¿—è¾“å‡ºï¼‰
- [ ] TensorBoard äº‹ä»¶æ–‡ä»¶å·²ç”Ÿæˆ
- [ ] å¥–åŠ±ä¸æ˜¯å…¨ 0 æˆ–å…¨ NaN
- [ ] è®­ç»ƒ 1000 æ­¥åæœ‰å¯è§çš„æ”¹è¿›
- [ ] æ¨¡å‹å¯ä»¥æˆåŠŸä¿å­˜å’ŒåŠ è½½

---

## ğŸ”— å…³é”®ç±»æ–¹æ³•

### StockTradingEnv
```python
env.reset()                    # é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹è§‚å¯Ÿ
env.step(action)               # æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (obs, reward, done, info)
env.render()                   # è¾“å‡ºå½“å‰çŠ¶æ€ï¼ˆç”¨äºè°ƒè¯•ï¼‰
env._take_action(action)       # æ‰§è¡Œäº¤æ˜“é€»è¾‘
env._next_observation()        # ç”Ÿæˆè§‚å¯Ÿ
```

### PPO æ¨¡å‹
```python
model.learn(total_timesteps)   # è®­ç»ƒ
model.predict(observation)    # æ¨ç†ï¼Œè¿”å› (action, state)
model.save(path)               # ä¿å­˜æ¨¡å‹
model = PPO.load(path, env)   # åŠ è½½æ¨¡å‹
```

### DummyVecEnv
```python
env.reset()                    # é‡ç½®ï¼Œè¿”å› (n_envs, obs_size) çš„è§‚å¯Ÿ
env.step(actions)              # æ­¥è¿›ï¼Œactions å½¢çŠ¶ (n_envs,)
env.render()                   # æ¸²æŸ“æ‰€æœ‰ç¯å¢ƒ
env.close()                    # å…³é—­ç¯å¢ƒ
```

---

## ğŸ’¾ æ–‡ä»¶æ“ä½œç¤ºä¾‹

### ä¿å­˜è®­ç»ƒç»“æœ
```python
import json
from datetime import datetime

results = {
    'timestamp': datetime.now().isoformat(),
    'total_return': 0.25,
    'sharpe_ratio': 1.5,
    'max_drawdown': 0.12,
    'config': {
        'learning_rate': 3e-4,
        'batch_size': 128,
        'total_timesteps': 50000
    }
}

with open('results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

### è®°å½•äº¤æ˜“æ—¥å¿—
```python
trades = []
for step in range(n_steps):
    action, _ = model.predict(obs)
    obs, reward, done, info = env.step(action)
    
    trades.append({
        'step': step,
        'action': action[0],
        'balance': env.balance,
        'net_worth': env.net_worth,
        'shares_held': env.shares_held,
        'reward': reward
    })

import pandas as pd
pd.DataFrame(trades).to_csv('trades.csv', index=False)
```

---

## ğŸ¯ å…¸å‹å·¥ä½œæµç¨‹

```
1ï¸âƒ£ æ•°æ®å‡†å¤‡
   â””â”€ åŠ è½½ AAPL.csv
   â””â”€ éªŒè¯æ•°æ®è´¨é‡
   â””â”€ æ•°æ®æ’åº

2ï¸âƒ£ ç¯å¢ƒè®¾ç½®
   â””â”€ åˆ›å»º StockTradingEnv
   â””â”€ å‘é‡åŒ–åŒ…è£…
   â””â”€ éªŒè¯ç©ºé—´

3ï¸âƒ£ æ¨¡å‹åˆ›å»º
   â””â”€ åˆå§‹åŒ– PPO
   â””â”€ è®¾ç½®è¶…å‚æ•°

4ï¸âƒ£ è®­ç»ƒæ‰§è¡Œ
   â””â”€ model.learn()
   â””â”€ ç›‘æ§ TensorBoard

5ï¸âƒ£ æ¨¡å‹è¯„ä¼°
   â””â”€ æ¨ç† 2000 æ­¥
   â””â”€ è®¡ç®—æŒ‡æ ‡

6ï¸âƒ£ ç»“æœä¿å­˜
   â””â”€ ä¿å­˜æ¨¡å‹
   â””â”€ ä¿å­˜äº¤æ˜“è®°å½•
   â””â”€ ç”ŸæˆæŠ¥å‘Š
```

---

## ğŸ“š å­¦ä¹ èµ„æºé“¾æ¥

- [Gymnasium å®˜æ–‡](https://gymnasium.farama.org/)
- [Stable Baselines3 æ–‡æ¡£](https://stable-baselines3.readthedocs.io/)
- [PPO è®ºæ–‡ (1707.06347)](https://arxiv.org/abs/1707.06347)
- [åŸå§‹ Medium æ•™ç¨‹](https://medium.com/@adamjking3/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e)

---

## ğŸ†˜ å¸¸è§å‘½ä»¤

```bash
# ç»ˆæ­¢è®­ç»ƒ
Ctrl+C

# æŸ¥çœ‹ tensorboard
tensorboard --logdir=tb_logs/

# åˆ é™¤æ—§æ—¥å¿—
rm -rf tb_logs/

# åˆ—å‡ºæ¨¡å‹æ–‡ä»¶
ls -lh models/

# æŸ¥çœ‹ Python ç‰ˆæœ¬
python --version

# åˆ—å‡ºåŒ…ç‰ˆæœ¬
pip show gymnasium stable-baselines3
```

---

**ç‰ˆæœ¬**: 1.0  
**é€‚ç”¨**: Stock Trading Environment v2  
**æ›´æ–°æ—¥æœŸ**: 2024å¹´
