"""
é…å¯¹äº¤æ˜“PPOæ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨ç¨³å®šåŸºçº¿3ï¼ˆStable-Baselines3ï¼‰åº“è®­ç»ƒé…å¯¹äº¤æ˜“ç­–ç•¥
"""

import os
import sys
import argparse
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path

# å¯¼å…¥å¿…è¦çš„åº“
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback

# å¯¼å…¥è‡ªå®šä¹‰ç¯å¢ƒ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from env.PairsTradingEnv import PairsTradingEnv


class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    
    def __init__(self, **kwargs):
        # æ–‡ä»¶è·¯å¾„
        self.data_path = kwargs.get('data_path', './data/pair_NINGDE_BYD.csv')
        self.models_dir = kwargs.get('models_dir', './models')
        self.logs_dir = kwargs.get('logs_dir', './logs/tb_logs')
        self.results_dir = kwargs.get('results_dir', './results')
        
        # è®­ç»ƒå‚æ•°
        self.total_timesteps = kwargs.get('total_timesteps', 100000)
        self.learning_rate = kwargs.get('learning_rate', 0.0001)
        self.batch_size = kwargs.get('batch_size', 64)
        self.n_steps = kwargs.get('n_steps', 2048)
        self.n_epochs = kwargs.get('n_epochs', 10)
        self.gamma = kwargs.get('gamma', 0.99)
        self.gae_lambda = kwargs.get('gae_lambda', 0.95)
        self.clip_range = kwargs.get('clip_range', 0.2)
        self.ent_coef = kwargs.get('ent_coef', 0.0)
        
        # æ•°æ®å‚æ•°
        self.train_test_split = kwargs.get('train_test_split', 0.8)
        self.initial_balance = kwargs.get('initial_balance', 10000.0)
        self.window_size = kwargs.get('window_size', 10)
        
        # æ£€æŸ¥ç‚¹
        self.checkpoint_interval = kwargs.get('checkpoint_interval', 10000)
        self.test_interval = kwargs.get('test_interval', 50000)
        
        # å…¶ä»–
        self.seed = kwargs.get('seed', 42)
        self.verbose = kwargs.get('verbose', 1)
    
    def __str__(self):
        return json.dumps(self.__dict__, indent=2)


def create_directories(config):
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    for directory in [config.models_dir, config.logs_dir, config.results_dir]:
        Path(directory).mkdir(parents=True, exist_ok=True)
    
    print(f"âœ“ åˆ›å»ºç›®å½•å®Œæˆ")
    print(f"  æ¨¡å‹ç›®å½•: {config.models_dir}")
    print(f"  æ—¥å¿—ç›®å½•: {config.logs_dir}")
    print(f"  ç»“æœç›®å½•: {config.results_dir}")


def load_and_split_data(config):
    """
    åŠ è½½æ•°æ®å¹¶æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    
    Args:
        config: è®­ç»ƒé…ç½®
        
    Returns:
        train_df, test_df: è®­ç»ƒé›†å’Œæµ‹è¯•é›†DataFrame
    """
    print("\n" + "=" * 70)
    print("æ•°æ®åŠ è½½ä¸åˆ’åˆ†")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    try:
        df = pd.read_csv(config.data_path)
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date').reset_index(drop=True)
        print(f"âœ“ åŠ è½½æ•°æ®æˆåŠŸ: {len(df)} è¡Œ")
        print(f"  æ—¥æœŸèŒƒå›´: {df['Date'].min().date()} åˆ° {df['Date'].max().date()}")
    except Exception as e:
        print(f"âœ— åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None, None
    
    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
    split_idx = int(len(df) * config.train_test_split)
    train_df = df.iloc[:split_idx].reset_index(drop=True)
    test_df = df.iloc[split_idx:].reset_index(drop=True)
    
    print(f"\nâœ“ æ•°æ®åˆ’åˆ†å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_df)} è¡Œ ({config.train_test_split*100:.0f}%)")
    print(f"    æ—¥æœŸ: {train_df['Date'].min().date()} åˆ° {train_df['Date'].max().date()}")
    print(f"  æµ‹è¯•é›†: {len(test_df)} è¡Œ ({(1-config.train_test_split)*100:.0f}%)")
    print(f"    æ—¥æœŸ: {test_df['Date'].min().date()} åˆ° {test_df['Date'].max().date()}")
    
    return train_df, test_df


def create_env(df, config, env_id='train'):
    """
    åˆ›å»ºç¯å¢ƒ
    
    Args:
        df: æ•°æ®é›†
        config: è®­ç»ƒé…ç½®
        env_id: ç¯å¢ƒæ ‡è¯†ï¼ˆç”¨äºåŒºåˆ†è®­ç»ƒå’Œæµ‹è¯•ï¼‰
        
    Returns:
        vectorized_env: å‘é‡åŒ–çš„ç¯å¢ƒ
    """
    def _make_env():
        env = PairsTradingEnv(
            df=df,
            initial_balance=config.initial_balance,
            window_size=config.window_size
        )
        # æ·»åŠ Monitorç”¨äºè®°å½•ç»Ÿè®¡ä¿¡æ¯
        env = Monitor(env)
        return env
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    vec_env = DummyVecEnv([_make_env])
    
    print(f"âœ“ åˆ›å»º{env_id}ç¯å¢ƒå®Œæˆ")
    
    return vec_env


def train_model(train_env, config):
    """
    è®­ç»ƒPPOæ¨¡å‹
    
    Args:
        train_env: è®­ç»ƒç¯å¢ƒ
        config: è®­ç»ƒé…ç½®
        
    Returns:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    print("\n" + "=" * 70)
    print("æ¨¡å‹è®­ç»ƒ")
    print("=" * 70)
    
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  æ€»æ­¥æ•°: {config.total_timesteps:,}")
    print(f"  å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  æ‰¹å¤§å°: {config.batch_size}")
    print(f"  Næ­¥: {config.n_steps}")
    print(f"  Gamma: {config.gamma}")
    print(f"  GAE Lambda: {config.gae_lambda}")
    
    # åˆ›å»ºPPOæ¨¡å‹
    model = PPO(
        policy='MlpPolicy',
        env=train_env,
        learning_rate=config.learning_rate,
        batch_size=config.batch_size,
        n_steps=config.n_steps,
        n_epochs=config.n_epochs,
        gamma=config.gamma,
        gae_lambda=config.gae_lambda,
        clip_range=config.clip_range,
        ent_coef=config.ent_coef,
        tensorboard_log=config.logs_dir,
        verbose=config.verbose,
        seed=config.seed,
    )
    
    print(f"\nâœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"  ç­–ç•¥: MlpPolicy (å¤šå±‚æ„ŸçŸ¥æœº)")
    print(f"  å‚æ•°æ•°é‡: {sum(p.numel() for p in model.policy.parameters()):,}")
    
    # è®¾ç½®æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=config.checkpoint_interval,
        save_path=os.path.join(config.models_dir, 'checkpoints'),
        name_prefix='ppo_pairs_ckpt',
        save_replay_buffer=False,
        save_vecnormalize=False,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\n{'=' * 70}")
    print("å¼€å§‹è®­ç»ƒ...")
    print(f"{'=' * 70}\n")
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True
        )
        print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼")
        return model
    except Exception as e:
        print(f"\nâœ— è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def save_model(model, config):
    """
    ä¿å­˜æ¨¡å‹
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        config: è®­ç»ƒé…ç½®
    """
    print("\n" + "=" * 70)
    print("ä¿å­˜æ¨¡å‹")
    print("=" * 70)
    
    model_path = os.path.join(config.models_dir, 'ppo_pairs_trading')
    
    try:
        model.save(model_path)
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}.zip")
    except Exception as e:
        print(f"âœ— ä¿å­˜å¤±è´¥: {e}")


def test_model(model, test_df, config):
    """
    åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•æ¨¡å‹
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        test_df: æµ‹è¯•é›†
        config: è®­ç»ƒé…ç½®
        
    Returns:
        dict: æµ‹è¯•ç»“æœæŒ‡æ ‡
    """
    print("\n" + "=" * 70)
    print("æ¨¡å‹æµ‹è¯•")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    test_env = PairsTradingEnv(
        df=test_df,
        initial_balance=config.initial_balance,
        window_size=config.window_size
    )
    
    # è¿è¡Œæµ‹è¯•
    obs, _ = test_env.reset(seed=config.seed)
    
    total_reward = 0
    episode_rewards = []
    net_worths = []
    actions_taken = {0: 0, 1: 0, 2: 0}
    
    done = False
    step = 0
    max_steps = len(test_df) - config.window_size
    
    while step < max_steps and not done:
        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œ
        action, _ = model.predict(obs, deterministic=True)
        action = int(action)
        actions_taken[action] += 1
        
        obs, reward, terminated, truncated, info = test_env.step(action)
        total_reward += reward
        net_worths.append(info['net_worth'])
        
        done = terminated or truncated
        step += 1
    
    # è®¡ç®—æŒ‡æ ‡
    initial_balance = config.initial_balance
    final_net_worth = test_env.net_worth
    total_return = (final_net_worth - initial_balance) / initial_balance
    
    # ç®€å•çš„å¤æ™®æ¯”ç‡ï¼ˆåŸºäºæ¯æ—¥æ”¶ç›Šç‡ï¼‰
    returns = np.diff(net_worths) / np.array(net_worths[:-1])
    sharpe_ratio = (returns.mean() / (returns.std() + 1e-6)) * np.sqrt(252) if len(returns) > 1 else 0
    
    # æœ€å¤§å›æ’¤
    cummax = np.maximum.accumulate(net_worths)
    drawdown = (cummax - net_worths) / cummax
    max_drawdown = drawdown.max() if len(drawdown) > 0 else 0
    
    results = {
        'initial_balance': initial_balance,
        'final_net_worth': final_net_worth,
        'total_return': total_return,
        'total_return_pct': total_return * 100,
        'total_reward': total_reward,
        'steps': step,
        'max_net_worth': max(net_worths),
        'min_net_worth': min(net_worths),
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown * 100,
        'action_distribution': {
            'åšå¤šä»·å·®': actions_taken[0],
            'å¹³ä»“': actions_taken[1],
            'åšç©ºä»·å·®': actions_taken[2]
        }
    }
    
    # æ‰“å°ç»“æœ
    print(f"\næµ‹è¯•ç»“æœ:")
    print(f"  åˆå§‹èµ„é‡‘: Â¥{results['initial_balance']:.2f}")
    print(f"  æœ€ç»ˆå‡€èµ„äº§: Â¥{results['final_net_worth']:.2f}")
    print(f"  æ€»æ”¶ç›Šç‡: {results['total_return_pct']:.2f}%")
    print(f"  æ€»ç´¯è®¡å¥–åŠ±: {results['total_reward']:.6f}")
    print(f"  è¿è¡Œæ­¥æ•°: {results['steps']}")
    print(f"  æœ€å¤§å‡€èµ„äº§: Â¥{results['max_net_worth']:.2f}")
    print(f"  æœ€å°å‡€èµ„äº§: Â¥{results['min_net_worth']:.2f}")
    print(f"  å¤æ™®æ¯”ç‡: {results['sharpe_ratio']:.4f}")
    print(f"  æœ€å¤§å›æ’¤: {results['max_drawdown']:.2f}%")
    print(f"\nåŠ¨ä½œåˆ†å¸ƒ:")
    for action_name, count in results['action_distribution'].items():
        pct = count / results['steps'] * 100 if results['steps'] > 0 else 0
        print(f"  {action_name}: {count} ({pct:.1f}%)")
    
    return results


def plot_results(test_df, results, config):
    """
    ç»˜åˆ¶æµ‹è¯•ç»“æœ
    
    Args:
        test_df: æµ‹è¯•é›†
        results: æµ‹è¯•ç»“æœ
        config: è®­ç»ƒé…ç½®
    """
    print("\n" + "=" * 70)
    print("ç”Ÿæˆç»“æœå¯è§†åŒ–")
    print("=" * 70)
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # ç¬¬ä¸€ä¸ªå›¾ï¼šå‡€èµ„äº§å˜åŒ–
    ax1 = axes[0]
    dates = test_df['Date'].values[:results['steps']]
    
    ax1.axhline(y=config.initial_balance, color='gray', linestyle='--', 
               linewidth=1, alpha=0.7, label='åˆå§‹èµ„é‡‘')
    ax1.set_ylabel('å‡€èµ„äº§ï¼ˆå…ƒï¼‰', fontsize=11)
    ax1.set_title('æµ‹è¯•é›† - å‡€èµ„äº§å˜åŒ–', fontsize=12, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc='best')
    
    # ç¬¬äºŒä¸ªå›¾ï¼šZ-Scoreå’ŒåŠ¨ä½œ
    ax2 = axes[1]
    z_scores = test_df['zscore'].values[:results['steps']]
    ax2.plot(range(len(z_scores)), z_scores, color='blue', linewidth=1, alpha=0.7)
    ax2.axhline(y=2, color='red', linestyle='--', linewidth=1, alpha=0.5, label='è¶…ä¹°/è¶…å–é˜ˆå€¼')
    ax2.axhline(y=-2, color='red', linestyle='--', linewidth=1, alpha=0.5)
    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)
    ax2.set_xlabel('æ—¶é—´æ­¥', fontsize=11)
    ax2.set_ylabel('Z-Score', fontsize=11)
    ax2.set_title('Z-Scoreåºåˆ—ï¼ˆæ ‡å‡†åŒ–ä»·å·®ï¼‰', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='best')
    
    plt.tight_layout()
    
    output_path = os.path.join(config.results_dir, 'test_results.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ ç»“æœå›¾è¡¨å·²ä¿å­˜: {output_path}")
    
    plt.close()


def save_results(results, config):
    """
    ä¿å­˜æµ‹è¯•ç»“æœä¸ºJSON
    
    Args:
        results: æµ‹è¯•ç»“æœ
        config: è®­ç»ƒé…ç½®
    """
    output_path = os.path.join(config.results_dir, 'test_results.json')
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ“ ç»“æœå·²ä¿å­˜: {output_path}")
    except Exception as e:
        print(f"âœ— ä¿å­˜å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='é…å¯¹äº¤æ˜“PPOæ¨¡å‹è®­ç»ƒè„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python train_pairs.py --total-timesteps 100000 --learning-rate 0.0001
  python train_pairs.py --data-path ./data/pair_data.csv --seed 123
        """
    )
    
    # æ–‡ä»¶è·¯å¾„å‚æ•°
    parser.add_argument('--data-path', type=str, default='./data/pair_NINGDE_BYD.csv',
                       help='é…å¯¹æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--models-dir', type=str, default='./models',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--logs-dir', type=str, default='./logs/tb_logs',
                       help='TensorBoardæ—¥å¿—ç›®å½•')
    parser.add_argument('--results-dir', type=str, default='./results',
                       help='ç»“æœä¿å­˜ç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--total-timesteps', type=int, default=100000,
                       help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--learning-rate', type=float, default=0.0001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--batch-size', type=int, default=64,
                       help='æ‰¹å¤§å°')
    parser.add_argument('--n-steps', type=int, default=2048,
                       help='Næ­¥æ•°')
    parser.add_argument('--n-epochs', type=int, default=10,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--gamma', type=float, default=0.99,
                       help='æŠ˜æ‰£å› å­')
    parser.add_argument('--gae-lambda', type=float, default=0.95,
                       help='GAE Lambda')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train-test-split', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--initial-balance', type=float, default=10000.0,
                       help='åˆå§‹è´¦æˆ·ä½™é¢')
    parser.add_argument('--window-size', type=int, default=10,
                       help='è§‚å¯Ÿçª—å£å¤§å°')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--verbose', type=int, default=1,
                       help='æ—¥å¿—è¯¦ç»†ç¨‹åº¦')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig(**vars(args))
    
    # æ‰“å°é…ç½®
    print("\n")
    print("â•”" + "=" * 68 + "â•—")
    print("â•‘" + " é…å¯¹äº¤æ˜“PPOæ¨¡å‹è®­ç»ƒ ".center(68) + "â•‘")
    print("â•š" + "=" * 68 + "â•")
    print("\nè®­ç»ƒé…ç½®:")
    print(config)
    
    # åˆ›å»ºç›®å½•
    create_directories(config)
    
    # åŠ è½½å’Œåˆ’åˆ†æ•°æ®
    train_df, test_df = load_and_split_data(config)
    if train_df is None or test_df is None:
        return
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    train_env = create_env(train_df, config, env_id='è®­ç»ƒ')
    
    # è®­ç»ƒæ¨¡å‹
    model = train_model(train_env, config)
    if model is None:
        return
    
    # ä¿å­˜æ¨¡å‹
    save_model(model, config)
    
    # åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•
    results = test_model(model, test_df, config)
    
    # ç»˜åˆ¶ç»“æœ
    plot_results(test_df, results, config)
    
    # ä¿å­˜ç»“æœ
    save_results(results, config)
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  æ¨¡å‹: {os.path.join(config.models_dir, 'ppo_pairs_trading.zip')}")
    print(f"  ç»“æœ: {os.path.join(config.results_dir, 'test_results.json')}")
    print(f"  å›¾è¡¨: {os.path.join(config.results_dir, 'test_results.png')}")
    print(f"  æ—¥å¿—: {config.logs_dir}")
    print(f"\nğŸ“Š æŸ¥çœ‹TensorBoardæ—¥å¿—:")
    print(f"  tensorboard --logdir {config.logs_dir}")
    print()


if __name__ == "__main__":
    main()
